{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "# pip install git+https://github.com/zphang/transformers.git@llama_push\n",
        "\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "4s_A1-mJRKsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig"
      ],
      "metadata": {
        "id": "NYodRwh1Q1fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"drive/MyDrive/lora-alpaca\"  # top level folder"
      ],
      "metadata": {
        "id": "Z14Fg9qHXucw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "AL2c8oMSQzZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, MODEL_PATH, local_files_only=True)"
      ],
      "metadata": {
        "id": "e53-1TAwTno-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "req = input(\"Anweisung: \")\n",
        "\n",
        "PROMPT = (\n",
        "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request. Write in German.\"\n",
        "    \"### Instruction:\"\n",
        "    f\"{req}\"\n",
        "    \"### Response:\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        ")\n",
        "print(\"Generating...\")\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "for s in generation_output.sequences:\n",
        "    print(tokenizer.decode(s))"
      ],
      "metadata": {
        "id": "KmikHqCgRkx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "toIsmSIARuiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}