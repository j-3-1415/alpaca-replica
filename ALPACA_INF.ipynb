{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s_A1-mJRKsM"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install pip install accelerate\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYodRwh1Q1fS"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from peft import PeftModel\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z14Fg9qHXucw"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"drive/MyDrive/lora-alpaca\"  # top level folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL2c8oMSQzZi"
      },
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e53-1TAwTno-"
      },
      "outputs": [],
      "source": [
        "model = PeftModel.from_pretrained(model, MODEL_PATH, local_files_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmikHqCgRkx6"
      },
      "outputs": [],
      "source": [
        "req = input(\"Anweisung: \")\n",
        "\n",
        "PROMPT = (\n",
        "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request. Write in German.\"\n",
        "    \"### Instruction:\"\n",
        "    f\"{req}\"\n",
        "    \"### Response:\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        ")\n",
        "print(\"Generating...\")\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "for s in generation_output.sequences:\n",
        "    print(tokenizer.decode(s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toIsmSIARuiJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
